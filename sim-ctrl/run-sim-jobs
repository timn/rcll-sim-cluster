#!/usr/bin/env python3

from work_queue import WorkQueue
from pod_controller import PodController
from job_generator import JobGenerator
from config import Configuration

import os
import sys
import argparse
import time
import signal
import kubernetes
import traceback
from datetime import datetime, timedelta
from traceback import print_exc
from kubernetes.client import V1DeleteOptions
from kubernetes.watch import Watch

class SimController(object):
	def __init__(self, include_recently_failed=False, job_namespace=None, tournament=None, run_at_most=0,
				 retain_logs=False, logs_basedir=None):
		self.config	= Configuration()
		self.include_recently_failed = include_recently_failed
		if 'POD_NAME' in os.environ:
			self.pod_name = os.environ['POD_NAME']
		else:
			self.pod_name = None
		self.namespace = self.config.kube_namespace
		self.job_namespace = job_namespace or self.config.kube_namespace
		self.run_at_most = run_at_most
		self.retain_logs = retain_logs
		self.logs_basedir = logs_basedir
		self.quit = False

		self.tournament_regex = None
		if tournament is not None:
			self.tournament_regex = JobGenerator.id_regex(tournament)

		# The work queue will figure out a valid combination of MongoDB access
		# parameters, e.g., host/port, URI, or replica set discovery via DNS
		self.wq = WorkQueue(host=self.config.mongodb_host,
		                    port=self.config.mongodb_port,
		                    uri=self.config.mongodb_uri,
		                    srv_name=self.config.mongodb_rs_srv,
		                    database=self.config.mongodb_queue_db,
		                    replicaset=self.config.mongodb_rs,
		                    collection=self.config.mongodb_queue_col)

		signal.signal(signal.SIGTERM, self.term_handler)

	def __enter__(self):
		if self.job_namespace == "auto":
			if self.pod_name is None:
				raise Exception("Cannot use namespace auto determination if POD_NAME not set")
			name_prefix = "rcll-sim-"
			if "JOB_NAMESPACE_PREFIX" in os.environ:
				name_prefix = os.environ["JOB_NAMESPACE_PREFIX"]
			self.job_namespace = self.generate_namespace_name(name_prefix)
		if self.pod_name is not None:
			self.set_pod_label('job-namespace', self.job_namespace)
		self.job_namespace_created = False
		if self.job_namespace != "default":
			self.job_namespace_created = self.create_namespace()
		self.podctrl = PodController(self.config, namespace=self.job_namespace)
		self.initialized = True
		return self

	def __exit__(self, exc_type, exc_value, traceback):
		if self.job_namespace_created:
			self.delete_namespace()
		if self.pod_name is not None:
			self.set_pod_label('job-namespace', None)
		self.initialized = False

	def term_handler(self, signum, frame):
		print("TERM signal received")
		self.quit = True
		raise Exception("TERM")

	def generate_namespace_name(self, name_prefix):
		kube_config = kubernetes.config.load_incluster_config()
		core_api = kubernetes.client.CoreV1Api()
		batch_api = kubernetes.client.BatchV1Api()

		pod = core_api.read_namespaced_pod(self.pod_name, self.namespace)
		job_name = pod.metadata.labels["job-name"]
		job = batch_api.read_namespaced_job(job_name, self.namespace)

		parallelism = job.spec.parallelism
		label_selector_dict = dict(job.spec.selector.match_labels)
		label_selector_str = ",".join(["%s=%s" % (key,value) for (key,value) in label_selector_dict.items()])
		field_selector_str="status.phase=Running"

		num_tries = 0
		pods=[]
		print("Determining job pod set")
		while num_tries < 150:
			all_pods = core_api.list_namespaced_pod(self.namespace,
			                                        label_selector=label_selector_str,
			                                        field_selector=field_selector_str)
			pods = list(map(lambda p: (p.metadata.name, dict(p.metadata.labels)), all_pods.items))
			if len(pods) == parallelism and self.pod_name in [p[0] for p in pods]:
				break
			else:
				num_tries += 1
				time.sleep(1)

		pods.sort(key=lambda p: p[0])

		if len(pods) == parallelism:
			pods_without_namespace = sorted([p[0] for p in pods if 'job-namespace' not in p[1]])
			if self.pod_name not in pods_without_namespace:
				raise Exception("Self not included in list of pods without namespace")

			possible_namespaces = ["%s%s" % (name_prefix, i) for i in range(0, parallelism)]
			used_namespaces = [p[1]['job-namespace'] for p in pods if 'job-namespace' in p[1]]
			namespaces = sorted(list(set(possible_namespaces) - set(used_namespaces)))
			if len(namespaces) != len(pods_without_namespace):
				raise Exception("Inconsistent namespaces and pods without namespace")

			return namespaces[pods_without_namespace.index(self.pod_name)]
		else:
			raise Exception("Failed to determine all job pods")

	def create_namespace(self):
		print("Creating namespace '%s'" % self.job_namespace)
		kube_config = kubernetes.config.load_incluster_config()
		core_api = kubernetes.client.CoreV1Api()

		# Assume that the namespace may exist.
		# If it exists, just use it, otherwise, create it.
		exists = False
		existing_namespaces = core_api.list_namespace()
		for n in existing_namespaces.items:
			if n.metadata.name == self.job_namespace:
				exists = True
				print("  - using existing namespace")

		if not exists:
			manifest = {"kind": "Namespace", "apiVersion": "v1",
			            "metadata": { "name": self.job_namespace } }
			core_api.create_namespace(manifest)
			
		if "JOB_NAMESPACE_COPY_SECRETS" in os.environ:
			secrets = os.environ["JOB_NAMESPACE_COPY_SECRETS"].split(' ')
			for s in secrets:
				print("  - copying secret '%s' from '%s'" % (s, self.namespace))
				self.copy_secret(s, self.namespace, self.job_namespace)

		return not exists

	def delete_namespace(self):
		print("Deleting namespace '%s'" % self.job_namespace)
		kube_config = kubernetes.config.load_incluster_config()
		core_api = kubernetes.client.CoreV1Api()
		try:
			core_api.delete_namespace(name=self.job_namespace, body = V1DeleteOptions())
		except:
			pass

	def copy_secret(self, name, from_namespace, to_namespace):
		kube_config = kubernetes.config.load_incluster_config()
		core_api = kubernetes.client.CoreV1Api()
		secret = core_api.read_namespaced_secret(name, from_namespace)
		manifest = {
			"kind": "Secret",
			"apiVersion": "v1",
			"metadata": { "name": name },
			"type": secret.type,
			"data": dict(secret.data)
		}
		try:
			core_api.delete_namespaced_secret(name, to_namespace, body = V1DeleteOptions())
		except:
			pass
		core_api.create_namespaced_secret(to_namespace, manifest)


	def set_pod_label(self, label_key, label_value, pod_name = None):
		if pod_name is None:
			pod_name = self.pod_name

		if pod_name is None: return

		kube_config = kubernetes.config.load_incluster_config()
		core_api = kubernetes.client.CoreV1Api()
		patch = []
		if label_value is not None:
			patch.append({ "op": "add", "path": "/metadata/labels/%s" % label_key,
			               "value": label_value })
		else:
			patch.append({ "op": "remove", "path": "/metadata/labels/%s" % label_key })
		#print("Patch: %s" % str(patch))
		core_api.patch_namespaced_pod(pod_name, self.namespace, patch)

	def run(self):
		if not self.initialized:
			raise Exception("Must use 'with' statement to use instance")

		recently_failed_deadline=None
		if not self.include_recently_failed:
			recently_failed_deadline = datetime.utcnow() - timedelta(minutes=15)
		if self.tournament_regex is not None:
			print("Filtering job names by '%s'" % self.tournament_regex)
		jobs_run = 0
		job = None
		if not self.quit:
			job = self.wq.get_next_item(recently_failed_deadline, name_regex=self.tournament_regex)
		while job is not None:
			# We do +1 here to include the one we are currently handling
			(all_pending, without_recently_failed) = \
			    self.wq.num_pending_jobs(recently_failed_deadline, name_regex=self.tournament_regex)
			print("Open jobs: %d (additional recently failed: %d)" \
			      % (without_recently_failed+1, (all_pending-without_recently_failed)))

			try:
				print("Running job %s in namespace %s" % (job["name"], self.job_namespace))
				start_time = datetime.now()
				manifests=[]
				num_items = { "YAML": 0, "Pod": 0, "Container": 0,
				              "Service": 0, "Ingress": 0, "ConfigMap": 0,
							  "Role": 0, "RoleBinding": 0, "ServiceAccount": 0}
				for i in job["params"]["template_parameters"]:
					if not "vars" in i: i["vars"] = {}
					i["vars"]["namespace"] = self.job_namespace
					i["vars"]["job_name"] = job["name"]
					print("  - template: %s" % i["template"])
					sufficient_containers = i["sufficient_containers"] if "sufficient_containers" in i else []
					items = self.podctrl.create_from_template(i["template"], i["vars"],
					                                          sufficient_containers=sufficient_containers)
					for i in items:
						#print("    - %s: %s" % (i[0], i[1]))
						num_items[i[0]] += 1
						if i[0] == "Pod":
							num_items["Container"] += len(i[2]["spec"]["containers"])
						manifests.append(str(i[2]))

				update = { "$set": { "manifests": manifests } }
				self.wq.update_item(job["name"], update)

				format_string="Running {Pod} pods, {Container} cont, {Service} svc, " \
				               + "{Ingress} ing, {ConfigMap} cm"
				print(format_string.format(**num_items))

				print("Monitoring pods")
				if self.podctrl.monitor_pods():
					print("Job %s completed successfully" % job["name"])
					self.wq.mark_item_done(job["name"])
				else:
					print("Job %s failed, reqeueing" % job["name"])
					self.wq.requeue_item(job["name"])
			except:
				print("*** EXCEPTION ***")
				print_exc()
				print("Job %s failed, reqeueing" % job["name"])
				self.wq.requeue_item(job["name"])

			if self.retain_logs:
				# Store logs
				log_time_start = datetime.now()
				time_str = log_time_start.strftime("%Y%m%d-%H%M")
				job_str = job["name"].replace(":", "_")
				output_dir = os.path.normpath(os.path.join(self.logs_basedir, "%s-%s" % (job_str, time_str)))
				try:
					os.makedirs(output_dir)
					self.podctrl.download_all_pod_logs(output_dir, progress=False)
				except:
					print("Failed to download logs for %s (%s)" % (job["name"], str(sys.exc_info()[1])))
				log_time_end = datetime.now()
				print("Log download finished (took %s)\n" % str(log_time_end-log_time_start))

			# Cleanup removing pods, services
			self.podctrl.delete_all()
			end_time = datetime.now()
			print("Job %s finished (took %s)\n" % (job["name"], str(end_time-start_time)))

			if not self.include_recently_failed:
				recently_failed_deadline = datetime.utcnow() - timedelta(minutes=15)

			jobs_run += 1
			if not self.quit and (self.run_at_most == 0 or jobs_run < self.run_at_most):
				job = self.wq.get_next_item(recently_failed_deadline, name_regex=self.tournament_regex)
			else:
				job = None

		print("Done running jobs")

if __name__ == '__main__':
	parser = argparse.ArgumentParser(description='Run RCLL Cluster Sim Jobs')
	parser.add_argument('--include-recently-failed', action='store_true',
	                    help='Allow execution of recently failed jobs.')
	parser.add_argument('--retain-logs', action='store_true',
	                    help='Enable retaining log files')
	parser.add_argument('--logs-basedir',
	                    help='Base directory where to store retained logs.')
	parser.add_argument('--namespace',
	                    help='Namespace for created pods.')
	parser.add_argument('--tournament',
	                    help='Run only jobs for specified tournament.')
	parser.add_argument('--run-at-most', type=int, metavar="N",
	                    help='Run no more than N jobs')
	args = parser.parse_args()

	job_namespace = "default"
	if "JOB_NAMESPACE" in os.environ:
		job_namespace = os.environ["JOB_NAMESPACE"]
	if args.namespace:
		job_namespace = args.namespace

	tournament = None
	if "TOURNAMENT" in os.environ:
		tournament = os.environ["TOURNAMENT"]
	if args.tournament:
		tournament = args.tournament

	run_at_most = 0
	if "RUN_AT_MOST" in os.environ:
		run_at_most = int(os.environ["RUN_AT_MOST"])
	if args.run_at_most:
		run_at_most = args.run_at_most

	retain_logs = False
	if "RETAIN_LOGS" in os.environ:
		retain_logs = bool(os.environ["RETAIN_LOGS"] not in ["false", "no"])
	if args.retain_logs:
		retain_logs = args.retain_logs

	logs_basedir = ""
	if "LOGS_BASEDIR" in os.environ:
		logs_basedir = os.environ["LOGS_BASEDIR"]
	if args.logs_basedir:
		logs_basedir = args.logs_basedir

	if retain_logs:
		if logs_basedir == "":
			raise Exception("Logs basedir must be set if retaining losg is enabled")
		if not os.path.isdir(logs_basedir):
			raise Exception("Logs basedir is not a directory")
		if not os.access(logs_basedir, os.W_OK):
			raise Exception("Cannot write to logs basedir")

	include_recently_failed = False
	if "RUN_ALSO_RECENTLY_FAILED" in os.environ \
	and os.environ["RUN_ALSO_RECENTLY_FAILED"].lower() == "true":
		include_recently_failed = True
	if args.include_recently_failed:
		include_recently_failed = True

	with SimController(include_recently_failed, job_namespace,
	                   tournament=tournament, run_at_most=run_at_most,
					   retain_logs=retain_logs, logs_basedir=logs_basedir) \
	as sim_ctrl:
		try:
			sim_ctrl.run()
		except Exception as e:
			print("Failure running script, aborting")
			print(traceback.format_exc())
			sys.exit(-2)
